{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5c1e84a1",
      "metadata": {
        "id": "5c1e84a1"
      },
      "source": [
        "# Lab 03: TensorFlow vs. PyTorch\n",
        "- Train a model on MNIST in both TensorFlow and PyTorch, convert to TFLite and ONNX.  \n",
        "- Use tf.GradientTape for Tensorflow custom training loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c58bba",
      "metadata": {
        "id": "a1c58bba"
      },
      "source": [
        "## TensorFlow Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23ebc05e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23ebc05e",
        "outputId": "4754ed37-5fe3-4adc-e72c-8eb75fd31ab2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.8582 - loss: 0.5008\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.9554 - loss: 0.1521\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9686 - loss: 0.1055\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.9752 - loss: 0.0831\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9796 - loss: 0.0683\n",
            "TF Training time: 58.095083475112915 seconds\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9681 - loss: 0.1060\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.09224318712949753, 0.9718000292778015]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0   # Fill in normalization factor\n",
        "x_test = x_test / 255.0     # Fill in normalization factor\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),        # Fill input shape\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),  # Fill number of hidden neurons\n",
        "    tf.keras.layers.Dense(10, activation='softmax')  # Fill number of output neurons\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',       # Fill name of loss function\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "start = time.time()\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "end = time.time()\n",
        "print(f\"TF Training time: {end - start} seconds\")       # Output training time\n",
        "model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72743ab8",
      "metadata": {
        "id": "72743ab8"
      },
      "source": [
        "## Convert TensorFlow model to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be6ab50a",
      "metadata": {
        "id": "be6ab50a"
      },
      "outputs": [],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(\"model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57c00c95",
      "metadata": {
        "id": "57c00c95"
      },
      "source": [
        "## PyTorch Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "623dfb49",
      "metadata": {
        "id": "623dfb49"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
        "train_loader = DataLoader(datasets.MNIST(root='./data', train=True, transform=transform, download=True), batch_size=32)\n",
        "test_loader = DataLoader(datasets.MNIST(root='./data', train=False, transform=transform, download=True), batch_size=1000)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(___, ___)    # Fill correct input and output size\n",
        "        self.fc2 = nn.Linear(___, ___)    # Fill correct input and output size\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.___(x))    # Fill correct layer\n",
        "        return self.___(x)    # Fill correct layer\n",
        "\n",
        "model = Net()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "start = time.time()\n",
        "for epoch in range(5):\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "end = time.time()\n",
        "print(f\"PyTorch Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        output = model(x)\n",
        "        pred = output.argmax(1)\n",
        "        correct += (pred == y).sum().item()\n",
        "print(f\"Test accuracy: {correct / len(test_loader.dataset):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6dbdab0",
      "metadata": {
        "id": "f6dbdab0"
      },
      "source": [
        "## Convert PyTorch model to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WuMKMhHc8aLF",
      "metadata": {
        "id": "WuMKMhHc8aLF"
      },
      "outputs": [],
      "source": [
        "# Install ONNX\n",
        "!pip install onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09925e9a",
      "metadata": {
        "id": "09925e9a"
      },
      "outputs": [],
      "source": [
        "dummy_input = torch.randn(1, 784)\n",
        "torch.onnx.export(model, dummy_input, \"model.onnx\",\n",
        "                  input_names=[\"input\"], output_names=[\"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sv4P-dSS_GQB",
      "metadata": {
        "id": "sv4P-dSS_GQB"
      },
      "source": [
        "## TensorFlow custom training loop using tf.GradientTape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KH-sDlHq_Gdw",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH-sDlHq_Gdw",
        "outputId": "376aaaaf-4004-4057-cd93-39272abc3e56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.3622, Accuracy: 0.0625\n",
            "Step 100, Loss: 0.6295, Accuracy: 0.7296\n",
            "Step 200, Loss: 0.5923, Accuracy: 0.8033\n",
            "Step 300, Loss: 0.2255, Accuracy: 0.8341\n",
            "Step 400, Loss: 0.3326, Accuracy: 0.8506\n",
            "Step 500, Loss: 0.3347, Accuracy: 0.8618\n",
            "Step 600, Loss: 0.2799, Accuracy: 0.8718\n",
            "Step 700, Loss: 0.4746, Accuracy: 0.8785\n",
            "Step 800, Loss: 0.1422, Accuracy: 0.8848\n",
            "Step 900, Loss: 0.4233, Accuracy: 0.8890\n",
            "Step 1000, Loss: 0.0638, Accuracy: 0.8931\n",
            "Step 1100, Loss: 0.1412, Accuracy: 0.8964\n",
            "Step 1200, Loss: 0.1497, Accuracy: 0.8992\n",
            "Step 1300, Loss: 0.0914, Accuracy: 0.9011\n",
            "Step 1400, Loss: 0.2254, Accuracy: 0.9038\n",
            "Step 1500, Loss: 0.1605, Accuracy: 0.9064\n",
            "Step 1600, Loss: 0.2107, Accuracy: 0.9085\n",
            "Step 1700, Loss: 0.1934, Accuracy: 0.9108\n",
            "Step 1800, Loss: 0.0694, Accuracy: 0.9129\n",
            "Training Accuracy for epoch 1: 0.9146\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.1071, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.2680, Accuracy: 0.9477\n",
            "Step 200, Loss: 0.2089, Accuracy: 0.9468\n",
            "Step 300, Loss: 0.1341, Accuracy: 0.9466\n",
            "Step 400, Loss: 0.1481, Accuracy: 0.9487\n",
            "Step 500, Loss: 0.1131, Accuracy: 0.9494\n",
            "Step 600, Loss: 0.1686, Accuracy: 0.9492\n",
            "Step 700, Loss: 0.0364, Accuracy: 0.9504\n",
            "Step 800, Loss: 0.1131, Accuracy: 0.9508\n",
            "Step 900, Loss: 0.2583, Accuracy: 0.9510\n",
            "Step 1000, Loss: 0.1506, Accuracy: 0.9521\n",
            "Step 1100, Loss: 0.1435, Accuracy: 0.9524\n",
            "Step 1200, Loss: 0.0463, Accuracy: 0.9528\n",
            "Step 1300, Loss: 0.1768, Accuracy: 0.9533\n",
            "Step 1400, Loss: 0.0368, Accuracy: 0.9540\n",
            "Step 1500, Loss: 0.0482, Accuracy: 0.9541\n",
            "Step 1600, Loss: 0.2009, Accuracy: 0.9547\n",
            "Step 1700, Loss: 0.0607, Accuracy: 0.9552\n",
            "Step 1800, Loss: 0.0660, Accuracy: 0.9557\n",
            "Training Accuracy for epoch 2: 0.9560\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.0479, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0353, Accuracy: 0.9697\n",
            "Step 200, Loss: 0.1386, Accuracy: 0.9694\n",
            "Step 300, Loss: 0.0290, Accuracy: 0.9683\n",
            "Step 400, Loss: 0.0782, Accuracy: 0.9676\n",
            "Step 500, Loss: 0.0177, Accuracy: 0.9669\n",
            "Step 600, Loss: 0.0741, Accuracy: 0.9673\n",
            "Step 700, Loss: 0.3500, Accuracy: 0.9673\n",
            "Step 800, Loss: 0.0833, Accuracy: 0.9672\n",
            "Step 900, Loss: 0.0133, Accuracy: 0.9671\n",
            "Step 1000, Loss: 0.0302, Accuracy: 0.9676\n",
            "Step 1100, Loss: 0.0900, Accuracy: 0.9672\n",
            "Step 1200, Loss: 0.0393, Accuracy: 0.9676\n",
            "Step 1300, Loss: 0.1230, Accuracy: 0.9678\n",
            "Step 1400, Loss: 0.0473, Accuracy: 0.9679\n",
            "Step 1500, Loss: 0.0258, Accuracy: 0.9676\n",
            "Step 1600, Loss: 0.0565, Accuracy: 0.9678\n",
            "Step 1700, Loss: 0.0579, Accuracy: 0.9681\n",
            "Step 1800, Loss: 0.1678, Accuracy: 0.9685\n",
            "Training Accuracy for epoch 3: 0.9688\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.0185, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0954, Accuracy: 0.9743\n",
            "Step 200, Loss: 0.0333, Accuracy: 0.9756\n",
            "Step 300, Loss: 0.0256, Accuracy: 0.9747\n",
            "Step 400, Loss: 0.1036, Accuracy: 0.9736\n",
            "Step 500, Loss: 0.0498, Accuracy: 0.9719\n",
            "Step 600, Loss: 0.0403, Accuracy: 0.9729\n",
            "Step 700, Loss: 0.1298, Accuracy: 0.9731\n",
            "Step 800, Loss: 0.0695, Accuracy: 0.9732\n",
            "Step 900, Loss: 0.0408, Accuracy: 0.9736\n",
            "Step 1000, Loss: 0.0127, Accuracy: 0.9737\n",
            "Step 1100, Loss: 0.0401, Accuracy: 0.9741\n",
            "Step 1200, Loss: 0.0336, Accuracy: 0.9744\n",
            "Step 1300, Loss: 0.1022, Accuracy: 0.9747\n",
            "Step 1400, Loss: 0.1608, Accuracy: 0.9748\n",
            "Step 1500, Loss: 0.0398, Accuracy: 0.9748\n",
            "Step 1600, Loss: 0.1244, Accuracy: 0.9749\n",
            "Step 1700, Loss: 0.0266, Accuracy: 0.9751\n",
            "Step 1800, Loss: 0.0167, Accuracy: 0.9752\n",
            "Training Accuracy for epoch 4: 0.9753\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.0538, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0054, Accuracy: 0.9783\n",
            "Step 200, Loss: 0.0325, Accuracy: 0.9796\n",
            "Step 300, Loss: 0.0121, Accuracy: 0.9785\n",
            "Step 400, Loss: 0.0709, Accuracy: 0.9793\n",
            "Step 500, Loss: 0.0303, Accuracy: 0.9789\n",
            "Step 600, Loss: 0.0181, Accuracy: 0.9796\n",
            "Step 700, Loss: 0.0091, Accuracy: 0.9799\n",
            "Step 800, Loss: 0.0622, Accuracy: 0.9797\n",
            "Step 900, Loss: 0.1395, Accuracy: 0.9798\n",
            "Step 1000, Loss: 0.0754, Accuracy: 0.9794\n",
            "Step 1100, Loss: 0.0172, Accuracy: 0.9794\n",
            "Step 1200, Loss: 0.0623, Accuracy: 0.9792\n",
            "Step 1300, Loss: 0.0100, Accuracy: 0.9789\n",
            "Step 1400, Loss: 0.0105, Accuracy: 0.9790\n",
            "Step 1500, Loss: 0.0364, Accuracy: 0.9790\n",
            "Step 1600, Loss: 0.2085, Accuracy: 0.9790\n",
            "Step 1700, Loss: 0.1632, Accuracy: 0.9792\n",
            "Step 1800, Loss: 0.0754, Accuracy: 0.9791\n",
            "Training Accuracy for epoch 5: 0.9791\n",
            "\n",
            "TF Training time: 344.75 seconds\n",
            "Test Accuracy: 0.9724\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0   # Fill in normalization factor\n",
        "x_test = x_test / 255.0   # Fill in normalization factor\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Prepare datasets\n",
        "batch_size = 32         # Fill same batch size as in first TF example\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),    # Fill size\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),    # Fill number of neurons and activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')     # Fill number of neurons and activation\n",
        "])\n",
        "\n",
        "# Define loss, optimizer, and metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch, training=True)\n",
        "            loss = loss_fn(y_batch, logits)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        train_acc_metric.update_state(y_batch, logits)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E4Nlg4lb_qdW",
      "metadata": {
        "id": "E4Nlg4lb_qdW"
      },
      "source": [
        "## Performance Otimization with Graph Execution using @tf.function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jmu_hciK_qle",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jmu_hciK_qle",
        "outputId": "fc9502a7-abb7-4b1d-bc40-a7c12cb821ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.2920, Accuracy: 0.2188\n",
            "Step 100, Loss: 0.4769, Accuracy: 0.7358\n",
            "Step 200, Loss: 0.2769, Accuracy: 0.8089\n",
            "Step 300, Loss: 0.2126, Accuracy: 0.8365\n",
            "Step 400, Loss: 0.3269, Accuracy: 0.8522\n",
            "Step 500, Loss: 0.3170, Accuracy: 0.8662\n",
            "Step 600, Loss: 0.2814, Accuracy: 0.8748\n",
            "Step 700, Loss: 0.2204, Accuracy: 0.8822\n",
            "Step 800, Loss: 0.4692, Accuracy: 0.8876\n",
            "Step 900, Loss: 0.1966, Accuracy: 0.8923\n",
            "Step 1000, Loss: 0.4621, Accuracy: 0.8958\n",
            "Step 1100, Loss: 0.3286, Accuracy: 0.8994\n",
            "Step 1200, Loss: 0.1512, Accuracy: 0.9023\n",
            "Step 1300, Loss: 0.4084, Accuracy: 0.9042\n",
            "Step 1400, Loss: 0.2332, Accuracy: 0.9069\n",
            "Step 1500, Loss: 0.4809, Accuracy: 0.9087\n",
            "Step 1600, Loss: 0.1113, Accuracy: 0.9115\n",
            "Step 1700, Loss: 0.2357, Accuracy: 0.9136\n",
            "Step 1800, Loss: 0.3394, Accuracy: 0.9155\n",
            "Training Accuracy for epoch 1: 0.9172\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.0390, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.4901, Accuracy: 0.9493\n",
            "Step 200, Loss: 0.2235, Accuracy: 0.9512\n",
            "Step 300, Loss: 0.1108, Accuracy: 0.9525\n",
            "Step 400, Loss: 0.1321, Accuracy: 0.9533\n",
            "Step 500, Loss: 0.0258, Accuracy: 0.9538\n",
            "Step 600, Loss: 0.1134, Accuracy: 0.9537\n",
            "Step 700, Loss: 0.0266, Accuracy: 0.9543\n",
            "Step 800, Loss: 0.1165, Accuracy: 0.9542\n",
            "Step 900, Loss: 0.2418, Accuracy: 0.9548\n",
            "Step 1000, Loss: 0.1858, Accuracy: 0.9552\n",
            "Step 1100, Loss: 0.1111, Accuracy: 0.9551\n",
            "Step 1200, Loss: 0.1621, Accuracy: 0.9555\n",
            "Step 1300, Loss: 0.2607, Accuracy: 0.9551\n",
            "Step 1400, Loss: 0.0728, Accuracy: 0.9553\n",
            "Step 1500, Loss: 0.0480, Accuracy: 0.9554\n",
            "Step 1600, Loss: 0.0981, Accuracy: 0.9562\n",
            "Step 1700, Loss: 0.0313, Accuracy: 0.9567\n",
            "Step 1800, Loss: 0.1193, Accuracy: 0.9569\n",
            "Training Accuracy for epoch 2: 0.9572\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.0552, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0798, Accuracy: 0.9666\n",
            "Step 200, Loss: 0.1230, Accuracy: 0.9655\n",
            "Step 300, Loss: 0.3008, Accuracy: 0.9654\n",
            "Step 400, Loss: 0.4236, Accuracy: 0.9652\n",
            "Step 500, Loss: 0.1981, Accuracy: 0.9656\n",
            "Step 600, Loss: 0.1085, Accuracy: 0.9658\n",
            "Step 700, Loss: 0.0623, Accuracy: 0.9661\n",
            "Step 800, Loss: 0.1234, Accuracy: 0.9666\n",
            "Step 900, Loss: 0.2006, Accuracy: 0.9670\n",
            "Step 1000, Loss: 0.2152, Accuracy: 0.9669\n",
            "Step 1100, Loss: 0.2354, Accuracy: 0.9674\n",
            "Step 1200, Loss: 0.0575, Accuracy: 0.9671\n",
            "Step 1300, Loss: 0.0983, Accuracy: 0.9671\n",
            "Step 1400, Loss: 0.0175, Accuracy: 0.9673\n",
            "Step 1500, Loss: 0.5157, Accuracy: 0.9676\n",
            "Step 1600, Loss: 0.0608, Accuracy: 0.9680\n",
            "Step 1700, Loss: 0.1310, Accuracy: 0.9682\n",
            "Step 1800, Loss: 0.2706, Accuracy: 0.9684\n",
            "Training Accuracy for epoch 3: 0.9686\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.0576, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0531, Accuracy: 0.9765\n",
            "Step 200, Loss: 0.0592, Accuracy: 0.9759\n",
            "Step 300, Loss: 0.0538, Accuracy: 0.9755\n",
            "Step 400, Loss: 0.0183, Accuracy: 0.9752\n",
            "Step 500, Loss: 0.0895, Accuracy: 0.9746\n",
            "Step 600, Loss: 0.1324, Accuracy: 0.9745\n",
            "Step 700, Loss: 0.0285, Accuracy: 0.9741\n",
            "Step 800, Loss: 0.0297, Accuracy: 0.9735\n",
            "Step 900, Loss: 0.0953, Accuracy: 0.9734\n",
            "Step 1000, Loss: 0.2214, Accuracy: 0.9740\n",
            "Step 1100, Loss: 0.0041, Accuracy: 0.9743\n",
            "Step 1200, Loss: 0.0587, Accuracy: 0.9742\n",
            "Step 1300, Loss: 0.0256, Accuracy: 0.9737\n",
            "Step 1400, Loss: 0.1095, Accuracy: 0.9737\n",
            "Step 1500, Loss: 0.0135, Accuracy: 0.9736\n",
            "Step 1600, Loss: 0.0348, Accuracy: 0.9737\n",
            "Step 1700, Loss: 0.0656, Accuracy: 0.9738\n",
            "Step 1800, Loss: 0.1006, Accuracy: 0.9740\n",
            "Training Accuracy for epoch 4: 0.9740\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.0877, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0201, Accuracy: 0.9811\n",
            "Step 200, Loss: 0.0134, Accuracy: 0.9798\n",
            "Step 300, Loss: 0.3867, Accuracy: 0.9805\n",
            "Step 400, Loss: 0.0102, Accuracy: 0.9804\n",
            "Step 500, Loss: 0.0217, Accuracy: 0.9799\n",
            "Step 600, Loss: 0.1336, Accuracy: 0.9795\n",
            "Step 700, Loss: 0.0265, Accuracy: 0.9797\n",
            "Step 800, Loss: 0.0357, Accuracy: 0.9793\n",
            "Step 900, Loss: 0.0743, Accuracy: 0.9791\n",
            "Step 1000, Loss: 0.0114, Accuracy: 0.9790\n",
            "Step 1100, Loss: 0.0632, Accuracy: 0.9790\n",
            "Step 1200, Loss: 0.0058, Accuracy: 0.9788\n",
            "Step 1300, Loss: 0.0455, Accuracy: 0.9787\n",
            "Step 1400, Loss: 0.0635, Accuracy: 0.9786\n",
            "Step 1500, Loss: 0.0093, Accuracy: 0.9786\n",
            "Step 1600, Loss: 0.0278, Accuracy: 0.9787\n",
            "Step 1700, Loss: 0.1482, Accuracy: 0.9788\n",
            "Step 1800, Loss: 0.0164, Accuracy: 0.9790\n",
            "Training Accuracy for epoch 5: 0.9791\n",
            "\n",
            "TF Training time: 21.43 seconds\n",
            "Test Accuracy: 0.9702\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0   # Fill in normalization factor\n",
        "x_test = x_test / 255.0   # Fill in normalization factor\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Prepare datasets\n",
        "batch_size = 32\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),    # Fill size\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),    # Fill number of neurons and activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')     # Fill number of neurons and activation\n",
        "])\n",
        "\n",
        "# Define loss, optimizer, and metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "@tf.function  # compile the function into a graph\n",
        "def train_step(x_batch, y_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x_batch, training=True)\n",
        "        loss = loss_fn(y_batch, logits)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    train_acc_metric.update_state(y_batch, logits)\n",
        "    return loss\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}